{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sqlite3 (from versions: none)\n",
      "ERROR: No matching distribution found for sqlite3\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlite3 sentence_transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model_name = 'sentence-transformers/msmarco-distilbert-base-v3'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('scriptures.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the verse_chunks table if it doesn't exist\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS verse_chunks (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    verse_id INTEGER REFERENCES verses(id) ON DELETE CASCADE,\n",
    "    chunk_text TEXT,\n",
    "    embedding BLOB\n",
    ")\n",
    "''')\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Function to chunk text into smaller pieces\n",
    "def chunk_text(text, max_length=32):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "\n",
    "# Fetch all verses\n",
    "cursor.execute(\"SELECT id, scripture_text FROM verses\")\n",
    "verses = cursor.fetchall()\n",
    "\n",
    "# Process each verse\n",
    "for verse_id, scripture_text in verses:\n",
    "    chunks = chunk_text(scripture_text)\n",
    "    for chunk in chunks:\n",
    "        embedding = model.encode(chunk)\n",
    "        embedding_blob = sqlite3.Binary(np.array(embedding).tobytes())\n",
    "        \n",
    "        # Insert chunk and embedding into the database\n",
    "        cursor.execute(\"INSERT INTO verse_chunks (verse_id, chunk_text, embedding) VALUES (?, ?, ?)\", \n",
    "                       (verse_id, chunk, embedding_blob))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\code\\versio\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model_name = 'sentence-transformers/msmarco-distilbert-base-v3'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('scriptures.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the verse_chunks table if it doesn't exist\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS verse_chunks (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    verse_id INTEGER REFERENCES verses(id) ON DELETE CASCADE,\n",
    "    chunk_text TEXT,\n",
    "    embedding BLOB\n",
    ")\n",
    "''')\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Function to chunk text into overlapping pieces\n",
    "def chunk_text(text, max_length=32, overlap=16):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_length - overlap):\n",
    "        chunk = ' '.join(words[i:i + max_length])\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Fetch all verses\n",
    "cursor.execute(\"SELECT id, scripture_text FROM verses\")\n",
    "verses = cursor.fetchall()\n",
    "\n",
    "# Process each verse\n",
    "for verse_id, scripture_text in verses:\n",
    "    chunks = chunk_text(scripture_text)\n",
    "    for chunk in chunks:\n",
    "        embedding = model.encode(chunk)\n",
    "        embedding_blob = sqlite3.Binary(np.array(embedding).tobytes())\n",
    "        \n",
    "        # Insert chunk and embedding into the database\n",
    "        cursor.execute(\"INSERT INTO verse_chunks (verse_id, chunk_text, embedding) VALUES (?, ?, ?)\", \n",
    "                       (verse_id, chunk, embedding_blob))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'bm25_scores' has been created successfully.\n",
      "BM25 scores have been successfully computed and stored.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "\n",
    "# Connect to SQLite database\n",
    "connection = sqlite3.connect('scriptures.db')\n",
    "\n",
    "# Create bm25_scores table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bm25_scores (\n",
    "    verse_id INTEGER REFERENCES verses(id) ON DELETE CASCADE,\n",
    "    term TEXT,\n",
    "    score REAL,\n",
    "    PRIMARY KEY (verse_id, term)\n",
    ");\n",
    "\"\"\"\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(create_table_query)\n",
    "connection.commit()\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n",
    "\n",
    "print(\"Table 'bm25_scores' has been created successfully.\")\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, k1=1.5, b=0.75, avg_doc_length=100.0):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.avg_doc_length = avg_doc_length\n",
    "\n",
    "    def calculate_score(self, doc_length, term_frequency, doc_frequency, total_docs):\n",
    "        idf = math.log((total_docs - doc_frequency + 0.5) / (doc_frequency + 0.5) + 1)\n",
    "        tf = (term_frequency * (self.k1 + 1)) / (term_frequency + self.k1 * (1 - self.b + self.b * (doc_length / self.avg_doc_length)))\n",
    "        return idf * tf\n",
    "\n",
    "def tokenize(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.lower().translate(translator).split()\n",
    "\n",
    "def compute_bm25_scores(connection):\n",
    "    bm25 = BM25()\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Get total number of documents\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM verses\")\n",
    "    total_docs = cursor.fetchone()[0]\n",
    "\n",
    "    # Calculate average document length\n",
    "    cursor.execute(\"SELECT AVG(LENGTH(scripture_text)) FROM verses\")\n",
    "    avg_doc_length = cursor.fetchone()[0]\n",
    "    bm25.avg_doc_length = avg_doc_length\n",
    "\n",
    "    # Get document frequency for each term\n",
    "    cursor.execute(\"SELECT id, scripture_text FROM verses\")\n",
    "    verses = cursor.fetchall()\n",
    "\n",
    "    doc_frequencies = defaultdict(int)\n",
    "    term_frequencies = defaultdict(Counter)\n",
    "    doc_lengths = {}\n",
    "\n",
    "    for verse_id, scripture_text in verses:\n",
    "        terms = tokenize(scripture_text)\n",
    "        doc_lengths[verse_id] = len(terms)\n",
    "        term_frequencies[verse_id].update(terms)\n",
    "        for term in set(terms):\n",
    "            doc_frequencies[term] += 1\n",
    "\n",
    "    # Clear existing bm25_scores\n",
    "    cursor.execute(\"DELETE FROM bm25_scores\")\n",
    "\n",
    "    # Insert BM25 scores\n",
    "    for verse_id, terms in term_frequencies.items():\n",
    "        doc_length = doc_lengths[verse_id]\n",
    "        for term, term_frequency in terms.items():\n",
    "            doc_frequency = doc_frequencies[term]\n",
    "            score = bm25.calculate_score(doc_length, term_frequency, doc_frequency, total_docs)\n",
    "            cursor.execute(\"INSERT INTO bm25_scores (verse_id, term, score) VALUES (?, ?, ?)\", (verse_id, term, score))\n",
    "\n",
    "    connection.commit()\n",
    "\n",
    "# Connect to SQLite database\n",
    "connection = sqlite3.connect('scriptures.db')\n",
    "\n",
    "# Compute and store BM25 scores\n",
    "compute_bm25_scores(connection)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()\n",
    "\n",
    "print(\"BM25 scores have been successfully computed and stored.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\code\\versio\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u4f8d' in position 0: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token, \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 7\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtoken\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u4f8d' in position 0: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "with open(\"vocab.txt\", \"w\") as f:\n",
    "    for token, id in tokenizer.vocab.items():\n",
    "        f.write(f\"{token}\\t{id}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
